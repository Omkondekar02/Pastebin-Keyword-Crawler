{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e382dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26d9d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ed8e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"crawler.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6840c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PastebinCrawler:\n",
    "    \"\"\"A crawler for Pastebin that searches for specific keywords.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.archive_url = \"https://pastebin.com/archive\"\n",
    "        self.raw_paste_url = \"https://pastebin.com/raw/{}\"\n",
    "        self.output_file = \"keyword_matches.jsonl\"\n",
    "        \n",
    "        # Keywords to search for\n",
    "        self.crypto_keywords = [\n",
    "            \"crypto\", \"bitcoin\", \"ethereum\", \"blockchain\", \"btc\", \"eth\",\n",
    "            \"altcoin\", \"defi\", \"nft\", \"token\", \"coin\", \"wallet\", \"mining\"\n",
    "        ]\n",
    "        self.telegram_keywords = [\"t.me\"]\n",
    "        \n",
    "        # Headers to mimic a browser\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c733da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archive_pastes(self):\n",
    "        \"\"\"Scrape the Pastebin archive to get the latest 30 paste IDs.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.archive_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paste_links = soup.select('table.maintable tr td:nth-child(1) a')\n",
    "            \n",
    "            paste_ids = []\n",
    "            for link in paste_links:\n",
    "                href = link.get('href')\n",
    "                if href.startswith('/'):\n",
    "                    paste_ids.append(href[1:])  # Remove the leading '/'\n",
    "            \n",
    "            logging.info(f\"Found {len(paste_ids)} paste IDs from the archive\")\n",
    "            return paste_ids\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error fetching archive: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7598ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paste_content(self, paste_id):\n",
    "        \"\"\"Fetch the content of a paste by its ID.\"\"\"\n",
    "        url = self.raw_paste_url.format(paste_id)\n",
    "        try:\n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error fetching paste {paste_id}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b63ddd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keywords(self, content):\n",
    "        \"\"\"Check if the content contains any of the keywords.\"\"\"\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        keywords_found = []\n",
    "        \n",
    "        # Check for crypto keywords\n",
    "        for keyword in self.crypto_keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', content.lower()):\n",
    "                keywords_found.append(keyword)\n",
    "        \n",
    "        # Check for Telegram links\n",
    "        for keyword in self.telegram_keywords:\n",
    "            if keyword in content.lower():\n",
    "                keywords_found.append(keyword)\n",
    "        \n",
    "        return keywords_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f979cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_match(self, paste_id, keywords_found):\n",
    "        \"\"\"Save the paste information to the output file.\"\"\"\n",
    "        url = self.raw_paste_url.format(paste_id)\n",
    "        timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Create context message based on keywords\n",
    "        context_keywords = []\n",
    "        if any(k for k in keywords_found if k in self.crypto_keywords):\n",
    "            context_keywords.append(\"crypto-related content\")\n",
    "        if any(k for k in keywords_found if k in self.telegram_keywords):\n",
    "            context_keywords.append(\"Telegram links\")\n",
    "        \n",
    "        context = f\"Found {' and '.join(context_keywords)} in Pastebin paste ID {paste_id}\"\n",
    "        \n",
    "        match_data = {\n",
    "            \"source\": \"pastebin\",\n",
    "            \"context\": context,\n",
    "            \"paste_id\": paste_id,\n",
    "            \"url\": url,\n",
    "            \"discovered_at\": timestamp,\n",
    "            \"keywords_found\": keywords_found,\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "        \n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(json.dumps(match_data) + '\\n')\n",
    "        \n",
    "        logging.info(f\"Saved match for paste {paste_id} with keywords: {keywords_found}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27cc2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "        \"\"\"Run the crawler to find and save matching pastes.\"\"\"\n",
    "        # Create/clear the output file\n",
    "        with open(self.output_file, 'w'):\n",
    "            pass\n",
    "        \n",
    "        paste_ids = self.get_archive_pastes()\n",
    "        total_matches = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8820862",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PastebinCrawler' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     crawler \u001b[38;5;241m=\u001b[39m PastebinCrawler()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mcrawler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PastebinCrawler' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    crawler = PastebinCrawler()\n",
    "    crawler.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
